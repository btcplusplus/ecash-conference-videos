# Fedimint Consensus by joschisan
How to design a low latency Byzantine Fault Tolerant system for constant performance with respect to the number of nodes.
This presentation was recorded in October 2024 in Berlin, Germany during bitcoin++ conference, ecash edition.

[![Fedimint Consensus](https://img.youtube.com/vi/DqXsaGz-28E/0.jpg)](https://www.youtube.com/watch?v=DqXsaGz-28E)
## Video Summary generated by AI:
Exploring Fedimint Consensus: A New Era of Reliability and Performance in Distributed Systems
In a detailed discussion, the talk centers on Fedimint consensus and the innovative approaches that redefine distributed systemsâ€™ latency and reliability dynamics. The speaker dives deep into the underlying design principles of Fedimint and the performance trade-offs that make it unique.

### The Core Challenge of Consensus
At the heart of Fedimint lies the Atomic Broadcast Problem, a challenge that distributed systems have grappled with for decades. It involves achieving consensus on transaction ordering in a way that is both Byzantine fault-tolerant and reliable in unpredictable network conditions. In Fedimint's context, this means handling up to one-third of nodes being malicious or offline while maintaining system functionality.

### Transition from Honey Badger BFT to Aleph BFT
Initially, Fedimint used Honey Badger BFT, an asynchronous Byzantine fault-tolerant consensus mechanism. While effective in solving the Atomic Broadcast Problem, it had drawbacks, notably high latency caused by its reliance on voting rounds and a reliable networking layer. The system's latency often became prohibitive as it depended on the slowest guardian node, a critical limitation for fast payment systems.

To address these challenges, Fedimint transitioned to Aleph BFT, an algorithm leveraging directed acyclic graphs (DAGs) to represent communication history. This switch brought transformative benefits:

- Constant Ordering Latency: The latency is no longer proportional to the number of guardians, enabling seamless scaling.
- Reduced Dependency on Reliable Networking: Unlike its predecessor, Aleph BFT doesn't require ordered message delivery, making it more practical in real-world networks.
- Dynamic Adjustment to Latency Spikes: The algorithm adapts to latency spikes by dynamically recalibrating which guardians drive the system forward.
### The Role of Directed Acyclic Graphs (DAGs)
Aleph BFT constructs a DAG to maintain and track message history among guardians. Each guardian creates "units" that act as nodes in the graph. These units reference previous rounds, ensuring that each guardian's local view aligns with the global consensus. By optimizing latency to match the fastest subset of guardians, Aleph BFT ensures that latency spikes or slow connections in some nodes do not bottleneck the entire system.

### System Performance and Practical Benefits
The Fedimint system demonstrates consistent latency, typically five times the one-way network latency between guardians. This is achieved without needing reliable connections or unbounded message buffers. Importantly, guardians can rejoin the consensus seamlessly after being offline, reinforcing system resilience.

### Session Switching: A Necessary Trade-Off
To manage memory consumption, Fedimint requires periodic "session switches," wherein guardians synchronize and finalize transactions. While this causes a temporary latency spike, the impact is minimal and manageable. The trade-off enables sustainable performance without excessive resource usage.

### Real-World Applications
Fedimint's robust design caters to real-world deployments, such as guardians operating on residential internet connections. By tolerating latency spikes and dynamically adjusting performance thresholds, it ensures user transactions remain unaffected even during network congestion.

### Conclusion
Fedimint's evolution exemplifies the power of tailored algorithms in distributed systems. By replacing Honey Badger BFT with Aleph BFT, the system has achieved unparalleled reliability and low-latency performance. Its innovative use of DAGs and adaptability to varying network conditions make it a significant milestone in distributed consensus systems, paving the way for faster, more resilient e-cash and payment solutions.
